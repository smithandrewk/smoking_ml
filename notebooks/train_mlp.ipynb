{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.imports import *\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "data_dir = 'data/andrew-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1500,64)\n",
    "        self.do1 = nn.Dropout(p=.2)\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = relu(x)\n",
    "        # x = self.do1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  \n",
    "model = MLP().to(device=device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load('X_train.pt')\n",
    "y_train = torch.load('y_train.pt')\n",
    "X_test = torch.load('X_test.pt')\n",
    "y_test = torch.load('y_test.pt')\n",
    "\n",
    "trainloader = DataLoader(TensorDataset(X_train,y_train),batch_size=64,shuffle=True)\n",
    "devloader = DataLoader(TensorDataset(X_test,y_test),batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr = []\n",
    "loss_dev = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'model_0.184.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3,-1,len(trainloader))\n",
    "lrs = 10**lre\n",
    "lri = []\n",
    "lossi = []\n",
    "model.train()\n",
    "loss_tr_total = 0\n",
    "for i,(X_tr,y_tr) in tqdm(enumerate(trainloader)):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lrs[i])\n",
    "    X_tr,y_tr = X_tr.to(device),y_tr.to(device)\n",
    "    logits = model(X_tr)\n",
    "    loss = criterion(logits,y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_tr_total += loss.item()\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "plt.plot(lri,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "for i in tqdm(range(5)):\n",
    "    model.train()\n",
    "    loss_tr_total = 0\n",
    "    for (X_tr,y_tr) in trainloader:\n",
    "        X_tr,y_tr = X_tr.to(device),y_tr.to(device)\n",
    "        logits = model(X_tr)\n",
    "        loss = criterion(logits,y_tr)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tr_total += loss.item()\n",
    "    loss_tr.append(loss_tr_total/len(trainloader))\n",
    "    model.eval()\n",
    "    loss_dev_total = 0\n",
    "    for (X_dv,y_dv) in devloader:\n",
    "        X_dv,y_dv = X_dv.to(device),y_dv.to(device)\n",
    "        logits = model(X_dv)\n",
    "        loss = criterion(logits,y_dv)\n",
    "        loss_dev_total += loss.item()\n",
    "    loss_dev.append(loss_dev_total/len(devloader))\n",
    "    plt.plot(loss_tr[-30:])\n",
    "    plt.plot(loss_dev[-30:])\n",
    "    plt.savefig('loss.jpg')\n",
    "    plt.close()\n",
    "    print(f'Epoch {i} Train: {loss_tr_total/len(trainloader)} Dev: {loss_dev_total/len(devloader)}')\n",
    "    torch.save(model.state_dict(),f'model_{loss_dev[-1]:.3f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'model_0.184.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import cms\n",
    "# # test confusion matrices\n",
    "y_true = torch.Tensor()\n",
    "y_pred = torch.Tensor().cuda()\n",
    "for (X,y) in tqdm(devloader):\n",
    "    y_true = torch.cat([y_true,y.round()])\n",
    "    y_pred = torch.cat([y_pred,sigmoid(model(X.cuda())).round()])\n",
    "y_pred = y_pred\n",
    "cms(y_true=y_true,y_pred=y_pred.detach().cpu(),current_date=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().cuda()\n",
    "model.load_state_dict(torch.load('model-epoch9.pt'))\n",
    "logits = model(X.cuda())\n",
    "y_pred = nn.Sigmoid()(logits)\n",
    "y_pred = y_pred.detach().cpu()\n",
    "y_pred = y_pred.round()\n",
    "y_pred = torch.cat([torch.zeros(50,1),y_pred,torch.zeros(49,1)]).long()\n",
    "df['y_pred'] = y_pred*10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
